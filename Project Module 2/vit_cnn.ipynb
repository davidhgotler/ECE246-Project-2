{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_load import load_CIFAR10\n",
    "\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Versions of used packages:\n",
    "# python == 3.10.8\n",
    "# torch == 2.6.0\n",
    "# numpy == 1.26.4\n",
    "# transformers == 4.49.0\n",
    "# Pillow == 9.4.0\n",
    "# datasets == 3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section #1.1: Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================= #\n",
    "# It might take some time to downliad the dataset the first time\n",
    "# ======================================================================================================= #\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('benjamin-paine/imagenet-1k-64x64', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db284bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = 500\n",
    "idx = np.arange(0,50000,int(50000/data_num))\n",
    "data = ds[idx]\n",
    "data_num = len(data['image'])\n",
    "print('data num =', data_num)\n",
    "\n",
    "plt.imshow(data[\"image\"][250]) #Pillow == 9.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================= #\n",
    "# Load the ViT model. See the original paper at: https://arxiv.org/abs/2010.11929\n",
    "# Documentation of the model can be found at: https://huggingface.co/docs/transformers/model_doc/vit\n",
    "# ======================================================================================================= #\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation='eager')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b196ee",
   "metadata": {},
   "source": [
    "Complete the following code to evaluate the validation accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# In this block, we evaluate the accuracy on the validation set of the ViT model.\n",
    "# To predict the label of an image from the validation set, you need to\n",
    "#   1) Process the image with\n",
    "#       inputs = image_processor(image, return_tensors=\"pt\")\n",
    "#   2) Get the classification score (before softmax) with\n",
    "#       model(**inputs).logits\n",
    "#   3) A pytorch tensor will be returned. Predict the label with the largest score.\n",
    "#   4) Evaluate the accuracy on the validation set\n",
    "# ==================================================================================== #\n",
    "\n",
    "val_acc = 0\n",
    "\n",
    "for i in range(0, data_num):\n",
    "    if((i+1) % (data_num/100)==0):\n",
    "        print(f'Progress: {int((i+1)/data_num*100)}%', end='\\r')\n",
    "\n",
    "    inputs = image_processor(data['image'][i], return_tensors=\"pt\")\n",
    "    logits = vit_model(**inputs).logits\n",
    "\n",
    "    # top 1 prediction\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    \n",
    "    if(data['label'][i] == predicted_label):\n",
    "        val_acc += 1\n",
    "        \n",
    "print()\n",
    "val_acc /= data_num\n",
    "\n",
    "print(f\"Val acc = {val_acc*100:.2f}%\")\n",
    "\n",
    "#plt.imshow(ds[idx][\"image\"]) #Pillow == 9.4.0\n",
    "#plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b56a84a",
   "metadata": {},
   "source": [
    "# Section 1.2: Attention maps\n",
    "Now, let's look at the attention map of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# We download a single image with higher resolution from: \n",
    "#   https://huggingface.co/datasets/huggingface/cats-image\n",
    "# This should be a image of two cute cats.\n",
    "# ==================================================================================== #\n",
    "cat_img = load_dataset('huggingface/cats-image')['test']['image'][0]\n",
    "\n",
    "# Feed the image into the model so that we can get the attention maps\n",
    "inputs = image_processor(images=cat_img, return_tensors=\"pt\")\n",
    "outputs = vit_model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions \n",
    "\n",
    "# This should show a processed and cropped image from the original image\n",
    "plt.imshow(inputs['pixel_values'][0].permute(1, 2, 0)/2+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2c6c2",
   "metadata": {},
   "source": [
    "## Question:\n",
    "In this problem, the size of each input image is 224x224 pixels. To tokenize each image into patches with each patch with the size of 16x16 pixels. Particularly in the classification task, an additional CLS token is added to the input (this comes from the technique from BERT). The output of the Transformer corresponding to the CLS token will be fed into an MLP for further classifcation task.\n",
    "\n",
    "In this setting, what will be the total number of tokens for one image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a16bd",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "(Your answer here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270efd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# This show the attentions map of the first attention layer.\n",
    "# ==================================================================================== #\n",
    "\n",
    "print('Attention maps of the first attention layer:')\n",
    "ig, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(attentions[0][0, i, :, :].detach().cpu().numpy())\n",
    "    #ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e82287",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "(1) What is the size of one attention map in the figure above? What does each element on the attention map stand for? \n",
    "\n",
    "(2) There are 12 attention maps in one attention layer. How is this related to the multi-hear attention we learned in class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e89d87",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "(1) (Your answer here)\n",
    "\n",
    "(2) (Your answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# This show the attentions map of the last attention layer.\n",
    "# ==================================================================================== #\n",
    "\n",
    "print('Attention maps of the last attention layer:')\n",
    "ig, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(attentions[-1][0, i, :, :].detach().cpu().numpy())\n",
    "    #ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616641b5",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "(1) One advantage of transformers is that they can relate patches (tokens) that are far away from each other. Can we observe this from the attention maps in the last attention layer?\n",
    "\n",
    "(2) What's the difference between the attention maps in the first attention layer and the last attention layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e11521",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "(1) (Your answer here)\n",
    "\n",
    "(2) (Your answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# Here is another way to visualize the attention map. This is in the paper: https://arxiv.org/abs/2005.00928\n",
    "# There are also explanations at: https://medium.com/@nivonl/exploring-visual-attention-in-transformer-models-ab538c06083a\n",
    "# The so called attention rollout coonsiders the attention effect of all layers.\n",
    "# We look at the attention values from the CLS token to all the other 14x14 tokens. \n",
    "# Thus, we can show the attention effect in a 14x14 plot. \n",
    "# ==================================================================================== #\n",
    "\n",
    "\n",
    "from utils.attention_rollout import attention_rollout\n",
    "from PIL import ImageFilter, Image\n",
    "\n",
    "rollout = attention_rollout(attentions)\n",
    "\n",
    "num_of_patches = 14*14\n",
    "img_size = 224\n",
    "\n",
    "cls_attention = rollout[0, 1:, 0]  # Get attention values from [CLS] token to all patches\n",
    "cls_attention = 1 - cls_attention.reshape(int(np.sqrt(num_of_patches)), int(np.sqrt(num_of_patches)))\n",
    "\n",
    "# Normalize the attention map for better visualization\n",
    "cls_attention = (cls_attention - cls_attention.min()) / (cls_attention.max() - cls_attention.min())\n",
    "\n",
    "ig, axs = plt.subplots(1, 2, figsize=(8, 8))\n",
    "axs[0].imshow(inputs['pixel_values'][0].permute(1, 2, 0)/2+0.5)\n",
    "axs[1].imshow(cls_attention.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14893ad",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "The output of the Transformer corresponding to the CLS token is used for further classification taks through an MLP. By looking at the attention map above, what can you say about the results? Which tokens make more influence to the prediction result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc5b88",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "\n",
    "(Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4e689",
   "metadata": {},
   "source": [
    "# Section 2.1: ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50 \n",
    "import tensorflow as tf\n",
    "\n",
    "# import the ResNet-50 model\n",
    "rn50_model = ResNet50(weights='imagenet')\n",
    "\n",
    "# process the data\n",
    "inputs = []\n",
    "for img in data['image']:    \n",
    "    inputs.append( tf.image.resize(tf.keras.utils.img_to_array(img), size=[224,224]) )\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "# predict the labels with resnet50\n",
    "pred = rn50_model.predict(inputs)\n",
    "\n",
    "# print the val acc\n",
    "val_acc = np.mean( pred.argmax(-1) == np.array(data['label']))\n",
    "print(f\"Val acc = {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f2abb",
   "metadata": {},
   "source": [
    "The accuracy is much lower than what we expect from ResNet50 since we are testing on a donwsized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================== #\n",
    "# Here, we show the feature maps of ResNet50\n",
    "# By defult, we are showing the output of the second layer of ResNet-50\n",
    "# You can play around different layer according to the model structure we show in the next block\n",
    "# ==================================================================================== #\n",
    "\n",
    "layer_idx = 2\t\t# plot the output of the desired layer\n",
    "square = 8\t\t\t# plot all 64 maps in an 8x8 squares (depends on which layer we are showing)\n",
    "\n",
    "# set a model with the same begining layers as ResNet-50, end at the desired layer to show its output\n",
    "fm_model = tf.keras.Model(inputs=rn50_model.inputs, outputs=rn50_model.layers[layer_idx].output)\n",
    "\n",
    "# get the image\n",
    "cat_img = load_dataset('huggingface/cats-image')['test']['image'][0]\n",
    "img = np.expand_dims(tf.image.resize(tf.keras.utils.img_to_array(cat_img), size=[224,224]), axis=0)\n",
    "img = tf.keras.backend.eval( tf.keras.backend.constant(img) )\n",
    "\n",
    "# let the image run through the network\n",
    "feature_maps = fm_model.predict(img)\n",
    "\n",
    "# show the plos\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "\tfor _ in range(square):\n",
    "\t\t# specify subplot and turn of axis\n",
    "\t\tax = plt.subplot(square, square, ix)\n",
    "\t\tax.set_xticks([])\n",
    "\t\tax.set_yticks([])\n",
    "\t\t# plot filter channel in grayscale\n",
    "\t\tplt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "\t\tix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6fa92",
   "metadata": {},
   "source": [
    "Different subplots correspond to the output of different filters. Each filter might detect different features like edges, textures or specific shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e186257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(rn50_model.layers)):\n",
    "\tlayer = rn50_model.layers[i]\n",
    "\t# check for convolutional layer\n",
    "\tif 'conv' not in layer.name:\n",
    "\t\tcontinue\n",
    "\t# summarize output shape\n",
    "\tprint(i, layer.name, layer.output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
