{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Proposed by Vaswani et al. in their seminal 2017 paper *\"Attention Is All You Need\"*, the transformer architecture has become the dominant model for a wide range of tasks. It has largely replaced LSTMs in language processing and played a pivotal role in the revolution of large language models (LLMs). In fact, the \"T\" in ChatGPT stands for transformer.\n",
    "\n",
    "Transformers are not only useful for language, they have also proven highly effective in other modalities, including images. In this project, we will explore the building blocks of the Vision Transformer, focusing on the **self-attention mechanism** at its core. In the second part, we will experiment with a pre-trained model to gain a better understanding of its inner workings.\n",
    "\n",
    "**Please do not forget to comment on the sections with answer the question**. \n",
    "You can write your answers in the same cell as the question.\n",
    "\n",
    "This notebook is self-contained, but you can refer to the resources below for more detailed information:\n",
    "\n",
    "1. **Original Transformer Paper:** [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\n",
    "2. **Vision Transformer Paper:** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)\n",
    "3. **Transformer Tutorial:** [Basics of Transformer](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "4. **Stanford CS224n Notes (personal favorite):** [Self-Attention and Transformers (2023 Draft)](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Components of the Transformer\n",
    "\n",
    "In this section, we will review the components of the transformer architecture. Some functions are provided, while you are expected to implement the remaining ones. Please answer the questions along the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization (Patchifying an Image)\n",
    "\n",
    "In the transformer architecture, the input is partitioned into basic units called tokens. At the heart of the self-attention mechanism (which is the core of the transformer) is the ability to capture relationships between any two tokens in the input, enabling the discovery of **long-range dependencies**. For text, tokens typically represent consecutive characters. For simplicity, we can assume that each word in a sentence is a token.\n",
    "\n",
    "In this case, self-attention examines the relationship between each word and every other word in the sentence (including itself) to generate a new representation for that word based on the similarity scores.\n",
    "\n",
    "In the image domain, defining tokens is simpler. Given an image of size $H \\times W$, we first divide it into square patches of size $P \\times P$. Each patch is then flattened to obtain a vector, which becomes our token. The image is then represented by $N$ tokens, where\n",
    "\n",
    "$$\n",
    "N = \\frac{H \\times W}{P^2}\n",
    "$$\n",
    "\n",
    "Each token vector consists of $P^2 \\times C$ elements, with $C$ being the number of color channels in the image.\n",
    "\n",
    "As the first step in our implementation, let's take a look at the patchification of an image.\n",
    "\n",
    "**Note**:\n",
    "In practice, a class (CLS) token is appended to the input for classification tasks. We omit it for simplicity in this part, we will see more of this in the second part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "\n",
    "# Load Chelsea image\n",
    "image = data.chelsea()\n",
    "cropped_image = image[:300, :450, :]\n",
    "\n",
    "plt.imshow(cropped_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the image that will be processed by the transformer.\n",
    "The patches obtained by calling **patchify_image** function are given below with their numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_image(x_in, patch_size):\n",
    "    \"\"\"\n",
    "    Patch Tokenization function\n",
    "    # Input\n",
    "        x_in (np.ndarray) : Image tensor of shape (H,W,C) \n",
    "        patch_size (int): Patch Size\n",
    "    # Output\n",
    "        x_out (np.ndarray): Return a matrix of size (N,C*P^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    H,W,C = x_in.shape\n",
    "    N_h = H // patch_size\n",
    "    N_w = W // patch_size\n",
    "    x_out = np.zeros([N_h*N_w,C*patch_size**2])\n",
    "    \n",
    "    for i in range(N_h):\n",
    "        for j in range(N_w):\n",
    "            x_out[(i*N_w) + j,:] = (x_in[i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size,:]).flatten()\n",
    "    \n",
    "    return x_out\n",
    "\n",
    "\n",
    "def display_patches(tokens, patch_size, image_shape):\n",
    "    H, W, _ = image_shape\n",
    "    n_h = H // patch_size\n",
    "    n_w = W // patch_size\n",
    "    \n",
    "    # n_h rows, n_w columns\n",
    "    fig, axes = plt.subplots(n_h, n_w, figsize=(6, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        y = tokens[i, :].reshape((patch_size, patch_size, 3))\n",
    "        y = y.astype(np.uint8) # need to convert to int for display\n",
    "        ax.imshow(y, vmin=0, vmax=255)\n",
    "        ax.axis(\"off\")\n",
    "        # print the token number\n",
    "        ax.text(2, 4, str(i), fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.7)) \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Patchify\n",
    "patch_size = 50\n",
    "tokens = patchify_image(cropped_image, patch_size)\n",
    "\n",
    "# Display\n",
    "display_patches(tokens, patch_size, cropped_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in Transformers\n",
    "\n",
    "In the previous part, we obtained the tokens (image patches) that the transformer model takes as input. Transformer models operate by representing the information from each token based on its relation to every other token in the input. For instance, while processing the top-left patch (token 0), the model uses its relation with every other patch to obtain an updated representation for that patch. The same idea applies to text, where every token (word) in a sentence is processed by the transformer, taking into account the relationships between any pair of tokens. This mechanism is called **self-attention** and it enables the transformer to capture **long-term dependencies**.\n",
    "\n",
    "The self-attention layer is the most important component of the transformer architecture. Now, let’s take a closer look at how it works.\n",
    "\n",
    "Self-attention represents every element in the input (tokens initially, and feature vectors for the deeper layers in the network) as a weighted sum of the representations of all other elements in the sequence. This is achieved by computing three vectors for every element:\n",
    "\n",
    "1. **Key**: What a token offers to other tokens. (of length $d_k$)\n",
    "2. **Query**: What each token looks for in the other tokens. (of length $d_k$)\n",
    "3. **Value**: The representation of each token, which we sum over using the computed weights to get the final representations. (of length $d_v$)\n",
    "\n",
    "For the $i$-th element in the input sequence, the similarity score (weight) is computed by taking the dot product of its query vector $q_i$ with the key vectors of every token in the sequence (including itself):\n",
    "\n",
    "$$\n",
    "\\alpha_i = q_i \\cdot K^T\n",
    "$$\n",
    "\n",
    "where $K$ is the matrix containing the key vectors for all tokens in its rows.\n",
    "\n",
    "These scores are then scaled by the factor $ \\frac{1}{\\sqrt{d_k}} $ to keep the variance of the dot product in check:\n",
    "\n",
    "$$\n",
    "\\alpha'_i = \\frac{q_i \\cdot K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Finally, a softmax is applied to $ \\alpha'_i $ to obtain the attention weights:\n",
    "\n",
    "$$\n",
    "w_{ij} = \\frac{\\exp(\\alpha'_{ij})}{\\sum_{k} \\exp(\\alpha'_{ik})}\n",
    "$$\n",
    "\n",
    "The output representation $o_i$ for the $i$-th element is computed as the weighted sum of the value vectors $v_j$:\n",
    "\n",
    "$$\n",
    "o_i = \\sum_{j} w_{ij} \\, v_j\n",
    "$$\n",
    "\n",
    "A self-attention layer takes an input $X$ of dimension $(T, D)$, where $T$ is the sequence length and $D$ is the input dimension. It then uses three linear projection matrices $W_K$, $W_Q$, and $W_V$ to obtain the keys, queries, and values, respectively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K &= X W_K, \\quad \\text{dimensions } (T \\times d_k) \\\\\n",
    "Q &= X W_Q, \\quad \\text{dimensions } (T \\times d_k) \\\\\n",
    "V &= X W_V, \\quad \\text{dimensions } (T \\times d_v)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The dot products between the queries and keys can grow large in magnitude when $d_k$ is high. Dividing by $\\sqrt{d_k}$ prevents these values from becoming too large, which helps stabilize gradients during training and ensures that the softmax function does not enter regions with extremely small gradients. (Recall that softmax input taking too low/high values leads to small gradients)\n",
    "\n",
    "The output $o_i$ from the attention layer becomes the input to the next layer, which allows to build higher-level representations in the deeper layers.\n",
    "\n",
    "Now complete the following **dot_product_SA** function to implement the self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_SA(x,W_k,W_q,W_v):\n",
    "    \"\"\"\n",
    "    Self-attention function\n",
    "    # Input\n",
    "        x   (np.ndarray) : Input matrix of shape (T,D) \n",
    "        W_k (np.ndarray) : Linear projector for key (D,d_k) \n",
    "        W_q (np.ndarray) : Linear projector for query (D,d_k) \n",
    "        W_v (np.ndarray) : Linear projector for value (D,d_v)  \n",
    "    # Output\n",
    "        x_out (np.ndarray): Return a matrix of size (T,d_v)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "    \n",
    "    \n",
    "        \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the self-attention function\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dimensions\n",
    "T = 4\n",
    "D = 6\n",
    "d_k = 5\n",
    "d_v = 2\n",
    "\n",
    "# Create random input data\n",
    "x = np.random.randn(T, D)  # (T, D)\n",
    "\n",
    "# Generate weight matrices\n",
    "W_k = np.random.randn(D, d_k)  # (D, d_k)\n",
    "W_q = np.random.randn(D, d_k)  # (D, d_k)\n",
    "W_v = np.random.randn(D, d_v)  # (D, d_v)\n",
    "\n",
    "# Call SA function\n",
    "output = dot_product_SA(x, W_k, W_q, W_v)\n",
    "\n",
    "# Print the output\n",
    "print(\"Self-Attention Output:\")\n",
    "print(output) # (T,d_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct you should see this array as the output:\n",
    "\n",
    "[[-0.49973861 -2.29958198]  \n",
    " [-0.84494707  2.58606031]  \n",
    " [-0.42453007  0.37405675]  \n",
    " [-0.49994822 -2.30716273]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the question! (Q1)\n",
    "\n",
    "Assuming we can process a variable number of tokens as input, what are the benefits and downsides of using a smaller patch size (P) to patchify our input image ? Please name at least one advantage and disdvantage. \n",
    "\n",
    "### Answer:\n",
    "\n",
    "Advantage:\n",
    "- (Your answer here)\n",
    "\n",
    "Disadvantage:\n",
    "- (Your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding\n",
    "\n",
    "The self-attention mechanism is simple yet powerful, and it played a crucial role in the emergence of transformer-based LLMs like ChatGPT, which millions of students worldwide rely on to complete their homework every day.\n",
    "\n",
    "However, the way we implemented the tokenization process and self-attention layer is not suitable for processing texts (or images) because **we do not have a notion of positional closeness** in the current implementation.\n",
    "\n",
    "For example, consider the image patches we obtained earlier. If we apply our self-attention function to process them, we would get an output for each patch based solely on its relation to every other patch regardless of their original arrangement. <br>\n",
    "\n",
    "In other words, the current mechanism does not distinguish between the original order of the patches and an arbitrary shuffling of them. Thus, the transformer network **cannot differentiate between the original image patches and the shuffled ones below**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "arr = np.arange(54)\n",
    "rng.shuffle(arr)\n",
    "\n",
    "patch_size = 50\n",
    "tokens = patchify_image(cropped_image, patch_size)\n",
    "\n",
    "image = data.chelsea()\n",
    "cropped_image = image[:300, :450, :]\n",
    "\n",
    "display_patches(tokens[arr], patch_size, cropped_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both images and texts, it is crucial to incorporate positional information. The relationship between words (in text) or patches (in an image) depends not only on their content but also on their relative positions. Transformers achieve this by adding a **positional encoding** vector to each token in the input. The positional encoding is constructed using a sinusoidal function with varying frequencies across different hidden dimensions.\n",
    "\n",
    "For each token at position $pos$ in the sequence and each dimension $i$ in the hidden dimensions, the positional encoding is defined as:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{D}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{D}}}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $pos$ is the position index in the sequence (from $0$ to $T-1$ for an input with $T$ tokens).\n",
    "- $i$ is the dimension index (from $0$ to $D-1$).\n",
    "- $D$ is the total embedding dimension.\n",
    "\n",
    "The denominator term $10000^{\\frac{2i}{D}}$ scales the frequencies of the sine and cosine functions, ensuring that **each hidden dimension has a different periodicity**.\n",
    "\n",
    "Thus, the **positional encoding matrix** $PE$ has the same shape as the tokenized input matrix:\n",
    "\n",
    "$$\n",
    "PE \\in \\mathbb{R}^{T \\times D}\n",
    "$$\n",
    "\n",
    "where $T$ is the sequence length and $D$ is the embedding dimension.\n",
    "\n",
    "The final input to the transformer is the sum of the token embeddings and the positional encoding:\n",
    "\n",
    "$$\n",
    "X_{\\text{position-encoded}} = X_{\\text{tokenized}} + PE\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X_{\\text{tokenized}} \\in \\mathbb{R}^{T \\times D}$ is the tokenized input matrix.\n",
    "- $PE \\in \\mathbb{R}^{T \\times D}$ is the positional encoding matrix.\n",
    "- $X_{\\text{position-encoded}}$ is the final input fed into the transformer network.\n",
    "\n",
    "By incorporating positional encoding, transformers can now capture positional relationships between tokens, distinguishing between different word orders in sentences and spatial arrangements in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position embedding\n",
    "def pos_embed(T,D):\n",
    "    \"\"\"\n",
    "    Positional Encoding function\n",
    "    # Input\n",
    "         T: Sequence length\n",
    "         D: Hidden dimension\n",
    "    # Output\n",
    "        x_out (np.ndarray): Return a matrix of size (T,D) with positional embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_matrix = np.array([np.arange(T),]*D).transpose()\n",
    "    \n",
    "    denum_matrix = np.ones([T,D])*10000\n",
    "    denum_idx_matrix = (np.array([np.repeat(np.arange(D//2),2),]*T)*2)/D\n",
    "    denum_matrix = np.power(denum_matrix, denum_idx_matrix)\n",
    "    pe_matrix = np.divide(pos_matrix,denum_matrix)\n",
    "    \n",
    "    phase_shift = np.array([0,np.pi/2])\n",
    "    phase_shift = np.array([phase_shift,]*T)\n",
    "    phase_shift = np.tile(phase_shift, (1, D//2))\n",
    "    pe_matrix = pe_matrix + phase_shift\n",
    "    \n",
    "    return np.sin(pe_matrix)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get a display of the position encoding matrix for an input of $1000$ tokens with hidden dimension $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,D = 100,50\n",
    "    \n",
    "pe = pos_embed(T,D)\n",
    "\n",
    "plt.imshow(pe.T, aspect='auto', cmap='viridis')  # pe.T to get dimension on the y-axis\n",
    "plt.colorbar(label=\"Embedding Value\")\n",
    "plt.title(\"Positional Embeddings Heatmap\")\n",
    "plt.xlabel(\"Position index\")\n",
    "plt.ylabel(\"Dimension index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the question!  (Q2)\n",
    "\n",
    "Can you comment on the difference between the sinusodial functions in different dimensions ? Do you see any pattern for consecutive dimensions or can we say anything about the change in frequency as dimension index increases ?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "(Your answer here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we  covered most of the components required to build a transformer network. Next, we refine some details to complete our implementation.\n",
    "\n",
    "In practice, attention layers in transformers often use multiple self-attention *heads* concurrently. This is similar to how convolutional layers in CNNs use multiple kernels to extract various features from the input. By employing multiple self-attention heads, each attention head can learn to focus on different aspects of the input simultaneously.\n",
    "\n",
    "This approach is called **Multi-Head Attention**. Suppose we have an input \n",
    "$$\n",
    "X \\in \\mathbb{R}^{T \\times D},\n",
    "$$ \n",
    "where $T$ is the sequence length and $D$ is the input dimension.\n",
    "\n",
    "Each attention head independently transforms the input into key, query, and value triplets using its own projection matrices:\n",
    "$$ W_Q^i \\in \\mathbb{R}^{D \\times d_k}, $$\n",
    "$$ W_K^i \\in \\mathbb{R}^{D \\times d_k}, $$\n",
    "$$ W_V^i \\in \\mathbb{R}^{D \\times d_v}. $$\n",
    "\n",
    "For the $i$-th head, these projections are computed as:\n",
    "$$\n",
    "Q^i = X W_Q^i, \\quad K^i = X W_K^i, \\quad V^i = X W_V^i.\n",
    "$$\n",
    "\n",
    "Each head then computes its output $O^i \\in \\mathbb{R}^{T \\times d_v}$ using the self-attention mechanism described earlier. Once all $h$ heads have computed their outputs, we concatenate them along the feature dimension to obtain:\n",
    "$$\n",
    "O = \\text{concat}(O^1, O^2, \\dots, O^h),\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "O \\in \\mathbb{R}^{T \\times (h \\cdot d_v)}.\n",
    "$$\n",
    "\n",
    "Finally, this concatenated output is projected to the desired output dimension $d_{\\text{out}}$ using a linear projection matrix \n",
    "$$\n",
    "W_O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{\\text{out}}},\n",
    "$$\n",
    "resulting in the final output of the multi-head attention layer:\n",
    "$$\n",
    "\\text{MultiHead}(X) = O W_O.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the following **multi_head_attention** function given below by using **dot_product_SA** function you implemented earlier. **Be careful about the dimensions of the input elements, check the comments.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "def multi_head_attention(W_q,W_k,W_v,W_o,x):\n",
    "    \"\"\"\n",
    "    Multi head attention\n",
    "    h: head count\n",
    "    T: sequence length\n",
    "    D: hidden dimension (assumed to be equal for input and output to multi-head attention)\n",
    "    d_k: dimension of key and query\n",
    "    d_v: dimension of value\n",
    "    # Input\n",
    "         W_q: Tensor containing Wq matrices of shape (h,D,d_k)\n",
    "         W_k: Tensor containing Wk matrices of shape (h,D,d_k)\n",
    "         W_v: Tensor containing Wv matrices of shape (h,D,d_v)\n",
    "         W_o: Output porjection matrix shape (h*d_v, D)\n",
    "         x: Input shape (T,D)\n",
    "    # Output\n",
    "         x_out (np.ndarray): Return a matrix of size (T,D)\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "\n",
    "\n",
    "        \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "    \n",
    "    return x_out\n",
    "\n",
    "d_k = 15\n",
    "d_v = 10\n",
    "D = 3\n",
    "T = 2\n",
    "h = 5\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create random input data\n",
    "x = np.random.randn(T, D)  # (T, D)\n",
    "\n",
    "# Generate weight tensors\n",
    "W_k = np.random.randn(h, D, d_k)  # (h, D, d_k)\n",
    "W_q = np.random.randn(h, D, d_k)  # (h, D, d_k)\n",
    "W_v = np.random.randn(h, D, d_v)  # (h, D, d_v)\n",
    "W_o = np.random.randn(h*d_v,D)\n",
    "\n",
    "# get the output\n",
    "x_out = multi_head_attention(W_q,W_k,W_v,W_o,x)\n",
    "\n",
    "print(x_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct you should see this array as the output:\n",
    "\n",
    "[[ -6.58072435 -10.05748565  -1.5506343 ]  \n",
    " [ -4.79706338  -5.78697211  -6.05258561]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done with our implementation of the transformer. Let's put everything together to implement a single-layer transformer as shown in the figure below:\n",
    "\n",
    "<img src=\"transformer_encoder_only.png\" alt=\"Transformer (Encoder) Layer\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure, you see what is called a **transformer encoder** network. For simplicity, we will refer to the encoder layer as the transformer layer. The Vision Transformer is also built using encoder layers, so do not worry about the differences between encoder and decoder layers.\n",
    "\n",
    "In the Vision Transformer, we start with the patchified image that already includes positional embeddings. Recall that the positional embedding is added to the tokenized input, so that:\n",
    "\n",
    "$$\n",
    "X_{\\text{position-encoded}} = X_{\\text{tokenized}} + PE.\n",
    "$$\n",
    "\n",
    "Next, $X_{\\text{position-encoded}}$ is passed through the multi-head attention module to produce $X_{\\text{attention}}$:\n",
    "\n",
    "$$\n",
    "X_{\\text{attention}} = \\text{MultiHead}(X_{\\text{position-encoded}}).\n",
    "$$\n",
    "\n",
    "The *Add \\& Norm* block then uses a residual connection to add the original input of the multi-head attention to its output, and the result is normalized. This normalization is performed by **Layer Normalization**, which is similar to batch normalization but computes the normalization statistics over the features for each element individually. Although layer normalization is crucial for stable learning in transformers, we omit it in our implementation for simplicity. You can check the original paper if you are interested in learning more about layer normalization: [https://arxiv.org/pdf/1607.06450](https://arxiv.org/pdf/1607.06450).\n",
    "\n",
    "The output of the first *Add \\& Norm* block is denoted as $X_1$. This output is then fed into a feed-forward layer, which is implemented as a two-layer MLP with a ReLU nonlinearity. The feed-forward network is expressed as follows:\n",
    "\n",
    "$$\n",
    "X_{\\text{FF}} = \\text{ReLU}(X_1 \\cdot W_1 + b_1)\\cdot W_2 + b_2.\n",
    "$$\n",
    "\n",
    "Finally, another *Add \\& Norm* block is applied to combine $X_1$ and $X_{\\text{FF}}$, giving the final output of the transformer layer.\n",
    "\n",
    "A transformer network is obtained by concatenating multiple blocks like this. The $N$ in the figure represents $N$ transformer (encoder) layers, where the output of the first layer becomes the input to the second layer’s multi-head attention, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the implementation of the **transformer_layer** given below according to description given above. **Please check the comments inside the function for details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer layer (Ask them to implement this)\n",
    "def transformer_layer(x,W_q,W_k,W_v,W_o,W_1,W_2,b_1,b_2):\n",
    "    \"\"\"\n",
    "    Transformer (encoder) layer\n",
    "    \n",
    "    Takes a batch (B) of inputs (T,D) matrices, returns the output of transformer layer.\n",
    "    Each input matrix passes through multi-head attention -> Add -> FF -> Add\n",
    "    \n",
    "    Use the function you implemented earlier for multi-head attention.\n",
    "    \n",
    "    Beware we do not use normalization, we have only addition for the Add & Norm layers.\n",
    "    \n",
    "    Hint: You can use a for loop to process batch.\n",
    "    \n",
    "    B: batch_size\n",
    "    T: sequence length\n",
    "    D: hidden dimension (assumed to be equal for input and output)\n",
    "    \n",
    "    h: head count\n",
    "    d_k: dimension of key and query\n",
    "    d_v: dimension of value\n",
    "    \n",
    "    # Input\n",
    "         W_q: Tensor containing Wq matrices of shape (h,D,d_k)\n",
    "         W_k: Tensor containing Wk matrices of shape (h,D,d_k)\n",
    "         W_v: Tensor containing Wv matrices of shape (h,D,d_v)\n",
    "         W_o: Output porjection matrix shape (h*d_v, D)\n",
    "         W_1, W_2: Weight matrices in the feed-forward layer (D,D)\n",
    "         b_1, b_2: Bias vectors in the feed-forward layer (D)\n",
    "         x: Input shape (B,T,D)\n",
    "    # Output\n",
    "         x_out (np.ndarray): Return a tensor of shape (B,T,D)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "\n",
    "    \n",
    "        \n",
    "    # -----------------------------\n",
    "    #        YOUR CODE HERE\n",
    "    # -----------------------------\n",
    "        \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2 \n",
    "T = 3\n",
    "D = 2\n",
    "h = 4\n",
    "d_k = 10\n",
    "d_v = 15\n",
    "\n",
    "# Generate random input and parameters\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.randn(B,T,D)  # (B, T, D)\n",
    "\n",
    "# Generate weight tensors\n",
    "W_k = np.random.randn(h, D, d_k)  # (h, D, d_k)\n",
    "W_q = np.random.randn(h, D, d_k)  # (h, D, d_k)\n",
    "W_v = np.random.randn(h, D, d_v)  # (h, D, d_v)\n",
    "W_o = np.random.randn(h*d_v,D)\n",
    "W_1 = np.random.randn(D,D)\n",
    "W_2 = np.random.randn(D,D)\n",
    "b_1 = np.random.randn(D)\n",
    "b_2 = np.random.randn(D)\n",
    "\n",
    "\n",
    "# get the output\n",
    "x_out = transformer_layer(x,W_q,W_k,W_v,W_o,W_1,W_2,b_1,b_2)\n",
    "\n",
    "print(x_out[0])\n",
    "print(x_out[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct you should be getting the following two matrices:\n",
    "\n",
    "[[ 6.51401336  4.19475558]  \n",
    " [11.2861679   9.07559194]  \n",
    " [ 4.76125266  1.90389302]]    \n",
    "[[ 9.14781168  2.44186795]  \n",
    " [10.60588289  2.81959273]  \n",
    " [ 5.08703529  1.33589121]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know the fundamentals of transformer architecture and how it processes an image.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
